{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "pretrained_dazai_text_generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Y9bOzRjhDrgK"
      },
      "source": [
        "# 学習済みRNNで太宰治風の文章生成"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TW_QNbp2EHNQ"
      },
      "source": [
        "## ライブラリのインポート"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "colab_type": "code",
        "id": "whF2Otd2FX0O",
        "outputId": "1397c55b-8af0-4dca-8d6f-cea3b776ecfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.4.0\n"
          ]
        }
      ],
      "source": [
        "import random, pickle, math\n",
        "import numpy as np\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4X9ifeqfEYt5"
      },
      "source": [
        "## GPUを利用する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "colab_type": "code",
        "id": "5RnjWUYPFX0T",
        "outputId": "235e6250-c50f-4637-a542-c3358144e7f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nMUa3lipQBAm"
      },
      "source": [
        "## モデルの定義"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "btQVC8FwFX0X"
      },
      "outputs": [],
      "source": [
        "class MyLSTM(nn.Module):\n",
        "  def __init__(self, vocab_size, emb_dim, hidden_size, dropout=0.5,\n",
        "               embeddings=None, freeze=False, weight_tied=False):\n",
        "    super(MyLSTM, self).__init__()\n",
        "\n",
        "    if embeddings is not None:\n",
        "      weight_size = (vocab_size, emb_dim)\n",
        "      if embeddings.size() != weight_size:\n",
        "        raise ValueError(\n",
        "            f'Expected weight size {weight_size}, got {embeddings.size()}')\n",
        "      self.embed = nn.Embedding.from_pretrained(embeddings, freeze=freeze)\n",
        "    else:\n",
        "      self.embed = nn.Embedding(vocab_size, emb_dim)\n",
        "    \n",
        "    self.dropout1 = nn.Dropout(dropout)\n",
        "    self.lstm1 = nn.LSTM(emb_dim, hidden_size)\n",
        "    self.dropout2 = nn.Dropout(dropout)\n",
        "    self.lstm2 = nn.LSTM(hidden_size, emb_dim)\n",
        "    self.dropout3 = nn.Dropout(dropout)\n",
        "    self.linear = nn.Linear(emb_dim, vocab_size)\n",
        "\n",
        "    nn.init.normal_(self.embed.weight, std=0.01)\n",
        "\n",
        "    nn.init.normal_(self.lstm1.weight_ih_l0, std=1/math.sqrt(emb_dim))\n",
        "    nn.init.normal_(self.lstm1.weight_hh_l0, std=1/math.sqrt(hidden_size))\n",
        "    nn.init.zeros_(self.lstm1.bias_ih_l0)\n",
        "    nn.init.zeros_(self.lstm1.bias_hh_l0)\n",
        "\n",
        "    nn.init.normal_(self.lstm2.weight_ih_l0, std=1/math.sqrt(hidden_size))\n",
        "    nn.init.normal_(self.lstm2.weight_hh_l0, std=1/math.sqrt(hidden_size))\n",
        "    nn.init.zeros_(self.lstm2.bias_ih_l0)\n",
        "    nn.init.zeros_(self.lstm2.bias_hh_l0)\n",
        "\n",
        "    if weight_tied:\n",
        "      self.linear.weight = self.embed.weight\n",
        "    else:\n",
        "      nn.init.normal_(self.linear.weight, std=1/math.sqrt(emb_dim))\n",
        "\n",
        "    nn.init.zeros_(self.linear.bias)\n",
        "\n",
        "  def forward(self, input, hidden_prev):\n",
        "    if hidden_prev is None:\n",
        "      hidden1_prev, hidden2_prev = None, None\n",
        "    else:\n",
        "      hidden1_prev = hidden_prev[0:2]\n",
        "      hidden2_prev = hidden_prev[2:4]\n",
        "    \n",
        "    emb_out = self.embed(input)\n",
        "    emb_out = self.dropout1(emb_out)\n",
        "    lstm1_out, hidden1_next = self.lstm1(emb_out, hidden1_prev)\n",
        "    lstm1_out = self.dropout2(lstm1_out)\n",
        "    lstm2_out, hidden2_next = self.lstm2(lstm1_out, hidden2_prev)\n",
        "    lstm2_out = self.dropout3(lstm2_out)\n",
        "    output = self.linear(lstm2_out)\n",
        "\n",
        "    hidden_next = hidden1_next + hidden2_next\n",
        "    return output, hidden_next"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YVX3-lxkQtX4"
      },
      "source": [
        "## 辞書の読み込み"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "-CuMIYOIFX0a"
      },
      "outputs": [],
      "source": [
        "filename = './data/word2id_50k.pkl'\n",
        "with open(filename, 'rb') as f:\n",
        "  word_to_id = pickle.load(f)\n",
        "  id_to_word = {v: k for k, v in word_to_id.items()}\n",
        "\n",
        "  dict_len = len(word_to_id)\n",
        "  word_to_id['<unk>'] = dict_len\n",
        "  id_to_word[dict_len] = '<unk>'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dslB0JuyEduJ"
      },
      "source": [
        "## 文章生成"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "colab_type": "code",
        "id": "teACVS3lFX0e",
        "outputId": "d2629441-84d1-483c-ce28-3222838de631"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "私は、薄情な、外国の古典までの、成績を、信頼すると同時に、そうしてその後に、また一枚、顔が白くて、伴うのものなぞ過ぎています。\n",
            "僕はもう、いくら未だきらいなんですね。\n",
            "誰にも自信が無いんです。\n",
            "女には、この創作遊覧が私の理想名詞というところになる。\n",
            "しばらくしても民衆の嘆きもつまらなくなりました。\n",
            "「愛しているんですよ。\n",
            "僕は、いいから、あんな工合いなものをおっしゃるように、と驚きますからね。」\n",
            "と言い、それを借りているのである。\n",
            "「四人と五いうのがわかっているような事ばかり書いていますよ。」\n",
            "私には善い文才が欲しかった。\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def text_generate(model, start_ids, length=100, skip_ids=None,\n",
        "                  prob=True, top=None, seed=2020):\n",
        "  word_ids = []\n",
        "  word_ids += start_ids\n",
        "\n",
        "  random.seed(seed)\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    hidden = None\n",
        "    input_id = start_ids\n",
        "    while len(word_ids) < length:\n",
        "      input = torch.tensor(input_id, dtype=torch.long,\n",
        "                           device=device).view(1, -1).t().contiguous()\n",
        "      output, hidden = model(input, hidden)\n",
        "      \n",
        "      p_list = F.softmax(output[-1].flatten(), dim=0)\n",
        "\n",
        "      if top is not None:\n",
        "        sorted_p_list = p_list.sort(descending=True).values[:top]\n",
        "        sorted_idx = p_list.sort(descending=True).indices[:top]\n",
        "        p_list = sorted_p_list / sorted_p_list.sum()\n",
        "\n",
        "      if prob:\n",
        "        while True:\n",
        "          rnd = random.random()\n",
        "          p_sum = 0\n",
        "          for idx, p in enumerate(p_list):\n",
        "            p_sum += p.item()\n",
        "            if rnd < p_sum:\n",
        "              sampled = idx if top is None else sorted_idx[idx].item()\n",
        "              break\n",
        "          if (skip_ids is None) or (sampled not in skip_ids):\n",
        "            break\n",
        "      else:\n",
        "        if skip_ids is not None:\n",
        "          p_list[skip_ids] = 0\n",
        "          sampled = p_list.argmax().item()\n",
        "\n",
        "      word_ids.append(sampled)\n",
        "      input_id = sampled\n",
        "    \n",
        "  return word_ids\n",
        "\n",
        "\n",
        "save_path = './data/model/weight_emb_tied.pth'\n",
        "state_dict = torch.load(save_path, map_location=device)\n",
        "vocab_size, emb_dim = state_dict['embed.weight'].size()\n",
        "hidden_size = state_dict['lstm1.weight_ih_l0'].size()[1]\n",
        "model = MyLSTM(vocab_size, emb_dim, hidden_size)\n",
        "model.load_state_dict(state_dict)\n",
        "model.to(device)\n",
        "\n",
        "start_words = ['私', 'は']\n",
        "\n",
        "start_ids = []\n",
        "for start_word in start_words:\n",
        "  if start_word not in word_to_id:\n",
        "    raise KeyError(start_word + ' is not in the dictionary!')\n",
        "  start_ids.append(word_to_id[start_word])\n",
        "skip_ids = [word_to_id['<unk>']]\n",
        "\n",
        "\n",
        "word_ids = text_generate(model, start_ids, length=173,\n",
        "                        skip_ids=skip_ids, top=None, seed=1)\n",
        "text = ''.join([id_to_word[w_id] for w_id in word_ids])\n",
        "text = text.replace('。', '。\\n').replace('。\\n」', '。」\\n')\n",
        "print(text)"
      ]
    }
  ]
}